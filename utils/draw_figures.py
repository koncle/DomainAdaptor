#
import matplotlib.pyplot as plt
import numpy as np
import torch

from utils.visualize import *
# from utils.visualize import line_plot, LightColors, regression_plot


def performance_degradation():
    EM = [81.05, 82.22, 83.14, 83.79, 82.41, 79.10]
    EMt = [83.86, 84.13, 82.76, 74.04, 32.13, 18.66]
    SLR = [81.98, 83.10, 84.17, 83.53, 65.66, 43.08]
    losses = [EM, SLR, EMt]
    bar_plot(losses, offset_between_bars=0.02, ylim=[10, 90], )


def confience():
    import seaborn as sns
    sns.set(style="whitegrid")
    EM = [83.43, 83.59, 83.62, 83.64, 83.61, 83.57, 83.58]
    SLR = [83.43, 84.58, 84.47, 84.31, 84.11, 84.12, 84.06]
    # GEM_t = [83.43, 84.86, 85.06, 85.11, 85.12, 85.19, 85.16]
    GEM_t = [83.43, 85.14, 85.13, 85.10, 85.06, 85.06, 85.04]
    GEM_TTA = [83.43, 84.67, 84.78, 84.84, 84.88, 84.90, 84.91]
    GEM_SKD = [83.43, 84.08, 84.19, 84.24, 84.30, 84.34, 84.36]

    losses = [EM, SLR, GEM_t, GEM_SKD, GEM_TTA]
    size = 15
    line_plot(losses, line_names=['EM', 'SLR', 'GEM-T', 'GEM-SKD', 'GEM-Aug'],
              ylim=[83.2, 85.5],
              x_axis_names=['> {:.1f}'.format(i / 10) for i in range(10, 3, -1)],
              plot_text=False,
              text_x_offset=-0.28,
              text_y_offset=0.08,
              # line_width=1.5,
              marker_size=35,
              legend_bbox_to_anchor=(0.03, 1.1, 1, 0.2),
              legend_ncol=3,
              # text_size=10,
              title_size=15,
              label_size=size,
              tick_size=size,
              legend_size=size - 3,
              figsize=(6, 4),
              save_name='confidence_all.pdf',
              x_label='Confidence',
              y_label='Acc (%)',
              legenc_loc='upper left'
              )


from sklearn.linear_model import LinearRegression


def plot_class_Norm_and_number():
    PACS = {
        'Photo': [
            [1.9028, 1.8563, 1.8010, 1.5216, 1.8514, 1.3711, 1.6156],
            [1.8930, 1.8403, 1.8355, 1.4741, 1.8560, 1.3600, 1.5818],
            [1.8745, 1.8489, 1.8215, 1.4565, 1.8469, 1.3960, 1.5969],
            [1.8677, 1.8547, 1.8410, 1.4733, 1.8542, 1.3817, 1.5731],
            [1.9183, 1.8423, 1.8339, 1.4668, 1.8351, 1.3507, 1.5954], ],
        'Art': [
            [1.8215, 1.8255, 1.7559, 1.4271, 1.7996, 1.2741, 1.5355],
            [1.8233, 1.7930, 1.7804, 1.4035, 1.8169, 1.2620, 1.5087],
            [1.6958, 1.7165, 1.6942, 1.3536, 1.7472, 1.2534, 1.4549],
            [1.7138, 1.7367, 1.7039, 1.3845, 1.7432, 1.2534, 1.4620],
            [1.8373, 1.7935, 1.7908, 1.3979, 1.7906, 1.2564, 1.5389],
        ],
        'Cartoon': [
            [1.7414, 1.7330, 1.7288, 1.4632, 1.7296, 1.2927, 1.4909],
            [1.7409, 1.7071, 1.7410, 1.4238, 1.7568, 1.2995, 1.4673],
            [1.7475, 1.7093, 1.7410, 1.4310, 1.7693, 1.2989, 1.4809],
            [1.7315, 1.7251, 1.7631, 1.4452, 1.7472, 1.3041, 1.4764],
            [1.7340, 1.7013, 1.7477, 1.4369, 1.7238, 1.2714, 1.4909],
            [1.6692, 1.6662, 1.5748, 1.3593, 1.5885, 1.3846, 1.5822],
        ],
        'Sketch': [
            [1.6628, 1.6837, 1.5645, 1.4031, 1.6335, 1.3936, 1.6202],
            [1.6536, 1.6814, 1.5894, 1.3693, 1.6447, 1.4160, 1.6249],
            [1.6178, 1.6807, 1.5378, 1.3556, 1.6287, 1.4338, 1.5773],
            [1.6205, 1.7064, 1.6111, 1.3497, 1.6133, 1.4183, 1.5723],
        ]
    }

    VLCS = {
        'Caltech': [
            [1.0823, 1.2989, 1.1781, 1.0756, 1.1766],
            [1.2167, 1.5618, 1.4316, 1.1873, 1.4611],
            [1.1418, 1.3967, 1.2833, 1.0982, 1.3203],
            [1.1517, 1.4490, 1.3422, 1.1781, 1.3487],
            [1.2320, 1.6194, 1.4746, 1.2281, 1.5284],
        ],
        'Labelme': [
            [1.1760, 1.3904, 1.3177, 1.1895, 1.3229],
            [1.3197, 1.5878, 1.5744, 1.3183, 1.6299],
            [1.1547, 1.3103, 1.2696, 1.0934, 1.3009],
            [1.0751, 1.2173, 1.1700, 1.0458, 1.1520],
            [1.2530, 1.4928, 1.4846, 1.2489, 1.5292],
        ],
        'Pascal': [
            [1.0076, 1.2955, 1.1803, 0.9257, 1.1578],
            [0.9931, 1.2037, 1.1428, 0.8312, 1.0826],
            [1.1850, 1.8568, 1.6765, 1.1411, 1.7895],
            [0.8997, 1.0044, 1.0129, 0.7610, 0.9135],
            [0.9331, 1.0846, 1.0772, 0.7807, 0.9981],
        ],
        'Sun': [
            [1.3065, 1.6648, 1.4858, 1.3418, 1.6239],
            [1.3838, 1.8003, 1.5463, 1.3889, 1.7862],
            [1.0424, 1.0897, 1.0024, 0.9341, 1.0371],
            [1.1960, 1.4156, 1.2922, 1.1744, 1.3303],
            [1.2959, 1.6247, 1.4265, 1.2718, 1.6038],
        ]
    }

    OfficeHome = {
        'Art': [
            [1.4306, 1.5534, 1.2891, 1.4173, 1.3602, 1.4854, 1.4052, 1.2459, 1.4741,
             1.4794, 1.4014, 1.2456, 1.5741, 1.4712, 1.3610, 1.4656, 1.2966, 1.2616,
             1.4076, 1.4213, 1.3893, 1.5498, 1.4701, 1.5178, 1.5303, 1.4621, 1.4476,
             1.3403, 1.2352, 1.3720, 1.5117, 1.3563, 1.4771, 1.4301, 1.3406, 1.5345,
             1.4768, 1.3671, 1.4365, 1.3538, 1.3503, 1.3454, 1.3777, 1.4841, 1.3273,
             1.4192, 1.4689, 1.5079, 1.4154, 1.5001, 1.3619, 1.3000, 1.4125, 1.3671,
             1.3774, 1.4921, 1.4469, 1.3947, 1.3797, 1.3360, 1.3201, 1.2869, 1.3741,
             1.5054, 1.4243],
            [1.5106, 1.6271, 1.3452, 1.4959, 1.4145, 1.5238, 1.5254, 1.2412, 1.5071,
             1.5455, 1.4667, 1.3003, 1.6080, 1.5606, 1.4515, 1.5135, 1.3151, 1.2895,
             1.4907, 1.4904, 1.4633, 1.6284, 1.5442, 1.6155, 1.6152, 1.5318, 1.5341,
             1.3835, 1.2733, 1.4111, 1.5863, 1.4386, 1.5226, 1.4609, 1.4002, 1.5752,
             1.5643, 1.4087, 1.4915, 1.3982, 1.4094, 1.4172, 1.4361, 1.5550, 1.4191,
             1.4482, 1.5494, 1.5598, 1.4579, 1.5767, 1.4540, 1.3347, 1.4530, 1.4252,
             1.4840, 1.5264, 1.5635, 1.4320, 1.4315, 1.3815, 1.3312, 1.3315, 1.4204,
             1.6127, 1.5053],
            [1.5121, 1.6154, 1.3107, 1.4993, 1.4184, 1.5414, 1.4735, 1.2596, 1.5045,
             1.5602, 1.4593, 1.2751, 1.6348, 1.5605, 1.4444, 1.5191, 1.3383, 1.2664,
             1.4731, 1.4769, 1.4315, 1.6737, 1.5269, 1.6236, 1.6256, 1.5209, 1.5205,
             1.3599, 1.2732, 1.3935, 1.5801, 1.4201, 1.5095, 1.4637, 1.3992, 1.5607,
             1.5656, 1.4079, 1.5014, 1.4050, 1.4594, 1.3866, 1.4060, 1.5711, 1.4010,
             1.4604, 1.5172, 1.5507, 1.4772, 1.5942, 1.4369, 1.3527, 1.4649, 1.3986,
             1.4592, 1.5174, 1.5244, 1.4117, 1.4347, 1.3653, 1.3561, 1.3389, 1.4223,
             1.5658, 1.5085],
            [1.4804, 1.5816, 1.3057, 1.4745, 1.3889, 1.5087, 1.4721, 1.2272, 1.4849,
             1.5229, 1.4493, 1.2630, 1.6130, 1.5392, 1.3850, 1.5116, 1.3212, 1.2697,
             1.4726, 1.4545, 1.4310, 1.6055, 1.4777, 1.5835, 1.5697, 1.4839, 1.5205,
             1.3696, 1.2785, 1.3860, 1.5438, 1.3765, 1.4847, 1.4526, 1.3836, 1.5513,
             1.5423, 1.3645, 1.4682, 1.4024, 1.4425, 1.3839, 1.3949, 1.5430, 1.3679,
             1.4410, 1.5058, 1.5249, 1.4228, 1.5340, 1.4347, 1.2961, 1.4521, 1.3937,
             1.4504, 1.5137, 1.5192, 1.4334, 1.4107, 1.3539, 1.3320, 1.3243, 1.3975,
             1.5720, 1.4880],
            [1.5177, 1.5939, 1.3177, 1.5121, 1.4026, 1.5489, 1.4980, 1.2710, 1.5046,
             1.5446, 1.4501, 1.2653, 1.6504, 1.5559, 1.4342, 1.5258, 1.3317, 1.2883,
             1.5104, 1.4734, 1.4212, 1.6628, 1.5187, 1.6060, 1.5916, 1.4868, 1.5545,
             1.3962, 1.3022, 1.4071, 1.5923, 1.3985, 1.5169, 1.4651, 1.4282, 1.5981,
             1.5510, 1.4020, 1.4975, 1.4064, 1.4461, 1.3942, 1.4494, 1.5696, 1.4195,
             1.4788, 1.5177, 1.5600, 1.4422, 1.5623, 1.4513, 1.3264, 1.4792, 1.3903,
             1.4550, 1.5521, 1.5526, 1.4522, 1.4397, 1.3575, 1.3587, 1.3457, 1.4051,
             1.6103, 1.5200],
        ],
        'Clipart': [
            [1.4919, 1.4911, 1.2692, 1.4081, 1.4557, 1.5216, 1.3929, 1.1789, 1.4517,
             1.4588, 1.4112, 1.2593, 1.4704, 1.4401, 1.3447, 1.5092, 1.3506, 1.2810,
             1.4512, 1.4248, 1.4119, 1.5544, 1.4708, 1.4082, 1.5223, 1.4559, 1.3453,
             1.3800, 1.2447, 1.2933, 1.4842, 1.3477, 1.4554, 1.3387, 1.3570, 1.4628,
             1.4123, 1.4292, 1.3966, 1.4077, 1.4720, 1.2626, 1.4284, 1.5089, 1.2696,
             1.4721, 1.5832, 1.4440, 1.4729, 1.5430, 1.3903, 1.2680, 1.4617, 1.3546,
             1.4136, 1.4842, 1.4114, 1.3933, 1.3051, 1.3235, 1.3556, 1.2560, 1.4020,
             1.4903, 1.3502],
            [1.4864, 1.4705, 1.2794, 1.4229, 1.4604, 1.5235, 1.4076, 1.1870, 1.4524,
             1.4912, 1.4308, 1.2810, 1.4497, 1.4212, 1.3531, 1.5092, 1.3284, 1.2658,
             1.4666, 1.4530, 1.4185, 1.5712, 1.4525, 1.3999, 1.4768, 1.4521, 1.3886,
             1.3914, 1.2334, 1.2815, 1.4972, 1.3343, 1.4550, 1.3435, 1.3290, 1.4478,
             1.4299, 1.4552, 1.3700, 1.3893, 1.5043, 1.2592, 1.4210, 1.5029, 1.2886,
             1.4834, 1.5571, 1.4487, 1.4291, 1.5644, 1.4048, 1.2654, 1.4320, 1.3836,
             1.4704, 1.4723, 1.4217, 1.3876, 1.3141, 1.3512, 1.3224, 1.2759, 1.4148,
             1.4945, 1.3789],
            [1.4419, 1.4520, 1.2545, 1.4124, 1.4262, 1.4641, 1.3442, 1.1582, 1.4224,
             1.4442, 1.3984, 1.2450, 1.4390, 1.3883, 1.3103, 1.5020, 1.3308, 1.2889,
             1.4381, 1.3941, 1.4082, 1.4933, 1.4603, 1.3877, 1.4813, 1.4192, 1.3499,
             1.3886, 1.2329, 1.2687, 1.4387, 1.2942, 1.4399, 1.3261, 1.2961, 1.4690,
             1.3597, 1.4179, 1.3606, 1.3778, 1.4478, 1.2502, 1.4159, 1.4765, 1.2741,
             1.4253, 1.5242, 1.4601, 1.4376, 1.5478, 1.3288, 1.2259, 1.4331, 1.3415,
             1.4334, 1.4697, 1.3651, 1.3780, 1.3103, 1.2955, 1.3359, 1.2671, 1.4040,
             1.4819, 1.3531],
            [1.4812, 1.4798, 1.2730, 1.4122, 1.4725, 1.4656, 1.3578, 1.1890, 1.4323,
             1.4594, 1.4304, 1.2769, 1.4743, 1.4404, 1.3056, 1.5393, 1.3248, 1.2708,
             1.4671, 1.4162, 1.4188, 1.5536, 1.4607, 1.3931, 1.5147, 1.4283, 1.3855,
             1.4079, 1.2487, 1.2646, 1.4875, 1.3119, 1.4669, 1.3603, 1.3575, 1.4542,
             1.4196, 1.4407, 1.3728, 1.4097, 1.4726, 1.2412, 1.4369, 1.5037, 1.2971,
             1.4690, 1.5820, 1.4549, 1.4513, 1.5634, 1.4127, 1.2674, 1.4397, 1.3628,
             1.4409, 1.4920, 1.4259, 1.3968, 1.2984, 1.3577, 1.3782, 1.2827, 1.3942,
             1.4827, 1.3876],
            [1.3591, 1.3277, 1.1824, 1.2931, 1.3353, 1.3427, 1.2529, 1.1218, 1.3187,
             1.3396, 1.2927, 1.1769, 1.3458, 1.2979, 1.1804, 1.4036, 1.2382, 1.1977,
             1.3211, 1.2690, 1.2874, 1.3706, 1.3143, 1.2556, 1.3690, 1.3317, 1.2506,
             1.2871, 1.1803, 1.1698, 1.3197, 1.1932, 1.3653, 1.2661, 1.2202, 1.3448,
             1.2278, 1.3204, 1.2384, 1.2877, 1.3144, 1.1645, 1.3185, 1.3644, 1.2170,
             1.3566, 1.3962, 1.3242, 1.3380, 1.4130, 1.2769, 1.1563, 1.3188, 1.2914,
             1.3032, 1.3890, 1.2286, 1.2877, 1.2264, 1.2059, 1.2401, 1.2181, 1.3359,
             1.3527, 1.1941],
        ],
        'Product': [
            [1.5957, 1.4940, 1.2783, 1.4940, 1.5200, 1.4998, 1.4697, 1.2104, 1.5569,
             1.5657, 1.3591, 1.2428, 1.6357, 1.5635, 1.4059, 1.4258, 1.3241, 1.3097,
             1.4732, 1.4874, 1.4369, 1.6054, 1.4842, 1.6215, 1.5813, 1.5070, 1.4362,
             1.4065, 1.3660, 1.3452, 1.4717, 1.4033, 1.5591, 1.3927, 1.4035, 1.6020,
             1.5432, 1.4601, 1.5216, 1.4090, 1.5129, 1.3606, 1.4072, 1.5423, 1.4228,
             1.4871, 1.5625, 1.5453, 1.5957, 1.5439, 1.4029, 1.3284, 1.5103, 1.3767,
             1.4043, 1.4879, 1.5139, 1.4569, 1.3427, 1.3331, 1.3436, 1.3499, 1.3584,
             1.5084, 1.4330],
            [1.6254, 1.5128, 1.3125, 1.5259, 1.5143, 1.5273, 1.5042, 1.2315, 1.5228,
             1.5964, 1.3792, 1.2672, 1.6478, 1.5563, 1.4481, 1.4373, 1.2932, 1.3165,
             1.5072, 1.5269, 1.4817, 1.5911, 1.5149, 1.6377, 1.5859, 1.5187, 1.4706,
             1.4436, 1.3858, 1.3718, 1.5124, 1.4379, 1.5743, 1.4128, 1.3977, 1.6047,
             1.5300, 1.4764, 1.5422, 1.4171, 1.5010, 1.4007, 1.4044, 1.5579, 1.4402,
             1.4973, 1.5486, 1.6019, 1.5580, 1.5644, 1.4325, 1.3183, 1.5224, 1.4210,
             1.4480, 1.4984, 1.5100, 1.4557, 1.3704, 1.3486, 1.3740, 1.3710, 1.3474,
             1.5223, 1.4888],
            [1.5273, 1.4254, 1.2476, 1.4364, 1.4432, 1.4017, 1.3712, 1.2006, 1.4538,
             1.4696, 1.3225, 1.2163, 1.5785, 1.4867, 1.3004, 1.3925, 1.2922, 1.2712,
             1.4027, 1.4051, 1.3590, 1.4599, 1.4431, 1.5371, 1.4603, 1.4271, 1.4029,
             1.3676, 1.3245, 1.2878, 1.4120, 1.3454, 1.4806, 1.3343, 1.3150, 1.5255,
             1.4045, 1.3877, 1.4633, 1.3819, 1.3625, 1.3283, 1.3323, 1.4876, 1.3613,
             1.4449, 1.4283, 1.5192, 1.4515, 1.5053, 1.3114, 1.2549, 1.4609, 1.3366,
             1.3313, 1.4333, 1.4060, 1.3859, 1.3115, 1.2858, 1.2933, 1.3150, 1.3287,
             1.4077, 1.3873],
            [1.6375, 1.4858, 1.3047, 1.5370, 1.5231, 1.4837, 1.4537, 1.1937, 1.5644,
             1.5758, 1.3824, 1.2874, 1.6601, 1.5637, 1.3931, 1.4553, 1.3011, 1.3226,
             1.4933, 1.4985, 1.4600, 1.5775, 1.4947, 1.6342, 1.5800, 1.4972, 1.5030,
             1.4502, 1.3895, 1.3632, 1.5156, 1.4187, 1.5504, 1.4313, 1.4065, 1.6085,
             1.5629, 1.4565, 1.5603, 1.4367, 1.5132, 1.3545, 1.4217, 1.5436, 1.4345,
             1.5374, 1.5450, 1.5905, 1.5775, 1.5796, 1.4209, 1.3143, 1.4938, 1.3881,
             1.4323, 1.5174, 1.5226, 1.4812, 1.3414, 1.3302, 1.3783, 1.3668, 1.3423,
             1.5246, 1.4867],
            [1.6253, 1.5255, 1.3060, 1.5622, 1.5017, 1.4965, 1.4673, 1.2198, 1.5125,
             1.5860, 1.3731, 1.2521, 1.6301, 1.5574, 1.4272, 1.4275, 1.3073, 1.3079,
             1.4689, 1.4714, 1.4311, 1.6050, 1.4820, 1.6178, 1.5679, 1.4964, 1.4916,
             1.4427, 1.3974, 1.3634, 1.4886, 1.4367, 1.5586, 1.4324, 1.4124, 1.5912,
             1.5745, 1.4526, 1.5614, 1.4311, 1.4887, 1.3538, 1.4287, 1.5356, 1.4246,
             1.5362, 1.5415, 1.5928, 1.5773, 1.5370, 1.4170, 1.3055, 1.4816, 1.3947,
             1.3891, 1.4811, 1.5174, 1.4775, 1.3815, 1.3278, 1.3878, 1.3599, 1.3512,
             1.5601, 1.4753],
        ],
        'ReaWorld': [
            [1.6131, 1.6001, 1.2834, 1.5142, 1.5542, 1.5482, 1.4566, 1.2486, 1.5223,
             1.5867, 1.4223, 1.2420, 1.6419, 1.5481, 1.2938, 1.4827, 1.3647, 1.3134,
             1.4804, 1.5205, 1.4122, 1.6472, 1.5298, 1.5694, 1.6437, 1.4501, 1.4403,
             1.3880, 1.3505, 1.4145, 1.5522, 1.3928, 1.5609, 1.4367, 1.3122, 1.6116,
             1.5283, 1.4133, 1.4995, 1.4212, 1.5005, 1.3976, 1.4393, 1.4841, 1.4163,
             1.5091, 1.4984, 1.5175, 1.5955, 1.5866, 1.3434, 1.3726, 1.4830, 1.4486,
             1.3986, 1.5421, 1.5140, 1.3681, 1.4264, 1.3559, 1.3254, 1.4088, 1.4336,
             1.5182, 1.5248],
            [1.6257, 1.5797, 1.2959, 1.5111, 1.5626, 1.5595, 1.4731, 1.2450, 1.5164,
             1.6047, 1.4173, 1.2740, 1.6437, 1.5309, 1.3430, 1.4891, 1.3344, 1.3058,
             1.4806, 1.5330, 1.4304, 1.6420, 1.5163, 1.5645, 1.6222, 1.4608, 1.4872,
             1.4115, 1.3548, 1.3981, 1.5636, 1.4160, 1.5576, 1.4429, 1.3256, 1.6194,
             1.5286, 1.4097, 1.5048, 1.4100, 1.4848, 1.4270, 1.4432, 1.4903, 1.4238,
             1.4966, 1.5068, 1.5438, 1.5388, 1.5743, 1.3658, 1.3487, 1.4973, 1.4387,
             1.4174, 1.5547, 1.5216, 1.3562, 1.4529, 1.3348, 1.3390, 1.4227, 1.4110,
             1.5432, 1.5590],
            [1.5278, 1.5199, 1.2511, 1.4512, 1.4802, 1.4675, 1.3526, 1.2247, 1.4369,
             1.5139, 1.3883, 1.2032, 1.5906, 1.4780, 1.2348, 1.4229, 1.3287, 1.2719,
             1.4460, 1.4191, 1.3464, 1.5355, 1.4719, 1.4961, 1.5413, 1.3820, 1.4390,
             1.3629, 1.2956, 1.3346, 1.4746, 1.3348, 1.4835, 1.3661, 1.2567, 1.5493,
             1.4358, 1.3705, 1.4630, 1.4039, 1.4014, 1.3694, 1.3892, 1.4041, 1.3595,
             1.4549, 1.3704, 1.4858, 1.4513, 1.5407, 1.2538, 1.3261, 1.4380, 1.3940,
             1.3212, 1.4771, 1.4196, 1.3024, 1.3919, 1.2742, 1.2805, 1.3974, 1.3797,
             1.4714, 1.4727],
            [1.6318, 1.6024, 1.2955, 1.5497, 1.5459, 1.5290, 1.3859, 1.2235, 1.5121,
             1.6101, 1.4439, 1.2643, 1.6587, 1.5327, 1.3263, 1.5022, 1.3469, 1.3362,
             1.5029, 1.4920, 1.4145, 1.6095, 1.5172, 1.5917, 1.6044, 1.4573, 1.5054,
             1.4232, 1.3588, 1.3831, 1.5426, 1.3995, 1.5229, 1.4485, 1.3368, 1.6223,
             1.5383, 1.4172, 1.5333, 1.4238, 1.4926, 1.3898, 1.4440, 1.5008, 1.4259,
             1.5132, 1.5242, 1.5263, 1.5487, 1.5746, 1.3327, 1.3496, 1.4892, 1.4419,
             1.4124, 1.5457, 1.5252, 1.3767, 1.4267, 1.3277, 1.3568, 1.4061, 1.4048,
             1.5598, 1.5548],
            [1.5250, 1.4907, 1.2462, 1.4567, 1.4698, 1.4484, 1.3349, 1.1964, 1.3875,
             1.5238, 1.3530, 1.1808, 1.5324, 1.4521, 1.2700, 1.4275, 1.3002, 1.2574,
             1.3933, 1.3826, 1.3292, 1.5062, 1.4239, 1.4474, 1.4983, 1.3658, 1.4062,
             1.3513, 1.3175, 1.3168, 1.4422, 1.3352, 1.4395, 1.3793, 1.2480, 1.5439,
             1.4135, 1.3458, 1.4429, 1.3576, 1.3814, 1.3174, 1.3951, 1.4467, 1.3334,
             1.4382, 1.4099, 1.4870, 1.4762, 1.4974, 1.2717, 1.2957, 1.4356, 1.3918,
             1.2924, 1.4811, 1.4252, 1.3125, 1.3913, 1.2623, 1.2856, 1.3788, 1.3759,
             1.4824, 1.4295],
        ]
    }

    class_distritbuions = {
        'PACS': [
            [1385, 1305, 1244, 833, 1205, 595, 911],
            [1214, 1257, 1151, 835, 1204, 581, 895],
            [1205, 1075, 1096, 879, 1093, 587, 935],
            [861, 821, 730, 453, 650, 775, 1156]
        ],
        'VLCS': [
            [301, 1987, 1087, 344, 2800],
            [411, 1227, 1108, 362, 2543],
            [236, 1584, 870, 97, 2360],
            [453, 1421, 445, 370, 2524],
        ],
        'OfficeHome': [
            [207, 224, 185, 177, 196, 242, 109, 153, 150, 188, 169, 169, 215, 147, 150, 200, 163, 167, 162, 149, 134, 236, 123, 186, 210, 207, 161, 165, 226, 249, 244, 179, 167, 214, 144, 231, 174,
             156, 154, 228, 244, 199, 146, 152, 174, 154, 157, 201, 126, 251, 129, 138, 208, 206, 139, 249, 198, 156, 210, 157, 160, 241, 203, 206, 130],
            [215, 150, 170, 151, 203, 201, 94, 101, 154, 128, 149, 155, 165, 112, 126, 205, 175, 150, 149, 131, 139, 175, 111, 118, 148, 173, 133, 163, 197, 176, 184, 124, 175, 149, 124, 204, 114,
             165, 128, 223, 190, 151, 132, 143, 135, 148, 155, 145, 109, 245, 116, 108, 187, 220, 137, 171, 132, 132, 151, 135, 165, 229, 205, 166, 86],
            [247, 157, 134, 173, 188, 199, 89, 108, 153, 172, 98, 128, 207, 143, 101, 157, 144, 137, 114, 141, 133, 190, 109, 169, 173, 143, 133, 140, 251, 167, 173, 140, 190, 136, 114, 250, 168, 147,
             141, 180, 198, 142, 106, 117, 190, 141, 147, 190, 134, 238, 114, 93, 158, 197, 129, 176, 169, 105, 143, 109, 144, 238, 160, 159, 99],
            [234, 187, 141, 141, 214, 222, 85, 122, 121, 177, 142, 123, 187, 136, 114, 165, 145, 144, 126, 135, 117, 215, 101, 145, 195, 154, 131, 139, 200, 193, 188, 141, 157, 170, 97, 208, 147, 130,
             123, 176, 216, 156, 128, 119, 174, 139, 113, 160, 142, 238, 88, 132, 170, 194, 115, 197, 167, 112, 182, 118, 126, 258, 172, 178, 125]
        ]
    }

    colors = ['#5A9BD5', '#1B9E78', '#FF9966', '#ff585d', '#614ad3', '#feda77']
    datasets = [PACS, VLCS, OfficeHome]
    for dataset, (name, class_dist) in zip(datasets, class_distritbuions.items()):
        x_list = []
        y_list = []
        for i, (d_name, domain) in enumerate(dataset.items()):
            avg_class_norm = np.array(domain).mean(0)
            distribution = class_dist[i]

            x_list.append(avg_class_norm)
            y_list.append(distribution)

            plt.scatter(avg_class_norm, distribution, label=d_name, color=colors[i])
            # plt.plot()

        x, y = np.stack(x_list), np.stack(y_list)
        x, y = x.reshape(-1, 1), y.reshape(-1, 1)
        # x,y = avg_class_norm.reshape(-1, 1), np.array(distribution).reshape(-1, 1)
        reg = LinearRegression().fit(x, y)
        cof = reg.score(x, y)
        print(cof)

        x = np.linspace(x.min(), x.max())
        y = reg.predict(x.reshape(-1, 1)).reshape(-1)
        plt.plot(x, y, '-', label='regression', color=colors[4])

        size = 18
        # plt.yticks(size=size)
        plt.yticks(fontsize=20)
        plt.xticks(fontsize=size)
        plt.xlabel('Class Norm', size=size)
        plt.ylabel('Training Number', size=size)
        plt.title(name, size=size)
        plt.legend(loc="upper left", fontsize=size)
        plt.tight_layout()
        plt.savefig('class_num_{}.pdf'.format(name))
        plt.show()
        # break


def batch_size():
    AdaBN = [68.40, 74.57, 77.80, 79.64, 80.44, 80.77, 81.08]
    MixBN = [80.66, 82.59, 83.20, 83.33, 83.43, 83.43, 83.33]
    GEM_t = [80.11, 82.48, 83.69, 84.57, 85.16, 85.46, 85.78]
    GEM_SKD = [81.05, 82.42, 83.15, 83.68, 84.37, 85.05, 85.90, ]  # 4,8,16,32,64
    GEM_TTA = [81.39, 82.93, 83.56, 84.23, 84.93, 85.65, 86.28]
    data = [AdaBN, MixBN, GEM_t, GEM_SKD, GEM_TTA]
    baseline = [[0, 79.44], [512, 79.44]]
    # bbox_to_anchor=(0, 1.0, 0.87, 0.2), ncol=3)
    size = 20
    bar_plot(data,
             colors=LightColors[:5],
             x_axis_names=['{}'.format(2 ** i) for i in range(2, 9)],
             bar_names=['AdaBN', 'AdaMixBN', 'GEM-T', 'GEM-SKD', 'GEM-TTA'],
             x_label='Batch Size',
             y_label='Acc (%)',
             width_of_all_col=0.8,
             offset_between_bars=0.01,
             ylim=[68, 87],
             title_size=9.5,
             label_size=size,
             tick_size=size,
             legend_size=size - 7,
             legenc_loc='lower right',
             figsize=(8, 4.5),
             save_name='batch_size_all.pdf', show=False)
    plt.show()


def draw_losses_with_confidence():
    PACS = [
        [0.0000, 0.0000, 0.0000, 1.3747, 0.9150, 0.8721, 0.7653, 0.6867, 0.4651, 0.0235],
        [0.0000, 0.0000, 1.6259, 1.3658, 1.1449, 0.9347, 0.8486, 0.7167, 0.4958, 0.0459],
        [0.0000, 0.0000, 1.5979, 1.3681, 1.1572, 0.9389, 0.8437, 0.7180, 0.4931, 0.0510],
        [0.0000, 0.0000, 1.5804, 1.3796, 1.1466, 0.9420, 0.8386, 0.7100, 0.4931, 0.0594]
    ]
    VLCS = [
        [0.0000, 0.0000, 0.0000, 1.3329, 1.1812, 0.9344, 0.8466, 0.6761, 0.4752, 0.1017, ],
        [0.0000, 0.0000, 0.0000, 1.2515, 1.0543, 0.8620, 0.7940, 0.6675, 0.4807, 0.0911, ],
        [0.0000, 0.0000, 1.4425, 1.2674, 1.0913, 0.9061, 0.8235, 0.6940, 0.4962, 0.1131, ],
        [0.0000, 0.0000, 1.4425, 1.2696, 1.0858, 0.8770, 0.8007, 0.6784, 0.4875, 0.1163, ]
    ]
    OH = [
        [0.0000, 2.8796, 2.4503, 2.0886, 1.7490, 1.4600, 1.2577, 0.9668, 0.6666, 0.1410],
        [3.3726, 2.9102, 2.4622, 2.1055, 1.7747, 1.4397, 1.2424, 0.9750, 0.6643, 0.1547],
        [3.3726, 2.9063, 2.4455, 2.0770, 1.7153, 1.3805, 1.1961, 0.9417, 0.6404, 0.1284],
        [3.3726, 2.8931, 2.4342, 2.0524, 1.6797, 1.3589, 1.1731, 0.9232, 0.6313, 0.1168]
    ]
    MDN = [
        [4.0812, 3.0909, 2.6539, 2.2207, 1.8326, 1.5031, 1.2503, 0.9736, 0.6683, 0.1008, ],
        [4.0246, 3.0748, 2.6362, 2.2108, 1.8343, 1.5009, 1.2613, 0.9829, 0.6675, 0.1071, ],
        [4.0500, 3.0547, 2.6017, 2.1730, 1.7889, 1.4507, 1.2274, 0.9592, 0.6502, 0.0982, ],
        [4.0052, 3.0679, 2.6101, 2.1846, 1.8027, 1.4642, 1.2394, 0.9688, 0.6547, 0.1007, ]
    ]

    data = [d[-1] for d in [MDN, OH, VLCS, PACS]]
    size = 15
    line_plot(data,
              x_axis_names=['{:.1f}'.format(i / 10) for i in range(10)],
              line_names=['MDN', 'OH', 'VLCS', 'PACS'],
              x_label='Confidence',
              y_label='Loss',
              line_styles=['-'] * 4,
              line_width=2,
              marker_size=25,
              # width_of_all_col=0.8,
              # offset_between_bars=0.02,
              # ylim=[0, 5],
              plot_text=False,
              title_size=9.5,
              label_size=17,
              tick_size=15,
              legend_size=size - 3,
              figsize=(3, 3),
              save_name='losses.pdf', )


def different_T():
    # 1, 2, 4, 8, 12, 16, 20
    PACS = [83.50, 83.91, 84.97, 85.30, 85.30, 85.25, 85.21]
    VLCS = [76.67, 77.14, 77.68, 77.65, 77.59, 77.54, 77.50]
    OH = [65.30, 65.52, 65.32, 65.29, 65.34, 65.35, 65.34]
    # MDN = [66.20, 66.55, 66.07, 66.29,  66.36, 66.34, 66.33, ]

    bar_plot([PACS], ylim=[82, 86])


def draw_continuous_adaptation():
    # 0 -> 1, 2, 3, 12, 23, 13, shuffled, continuals,
    Tent = [24.48, 22.30, 32.37, 11.97, 32.07, 13.78, 11.35, 15.35, 16.81, 14.35, 8.06, 11.60, 22.02, 17.57, 23.44]
    Shuffled = [41.55, 46.52, 41.45, 43.25, 39.81, 41.61, 44.03, 37.98, 38.96, 37.74, 47.38, 47.07, 43.11, 38.81, 40.04]
    Ours = [27.45, 25.11, 35.27, 13.59, 36.04, 14.47, 12.13, 17.18, 17.56, 12.82, 7.53, 11.02, 22.09, 22.95, 25.32]
    data = [Tent, Shuffled, Ours]

    domains = [d.strip() for d in 'gauss. & shot  & imp. & def. & glass & mot. & zoom  & snow  & frost & fog   & bright & cont. & elas. & pixel. & jpeg '.split('&')]

    size = 18
    bar_plot(data,
             # x_axis_names=['{:.1f}'.format(i / 10) for i in range(10)],
             x_axis_names=domains,
             bar_names=['Tent', 'Tent(s)', 'Ours'],
             x_label='Corruptions',
             y_label='Acc (%)',
             width_of_all_col=0.7,
             offset_between_bars=0.03,
             # ylim=[0, 5],
             title_size=9.5,
             label_size=size - 1,
             tick_size=size - 3,
             legend_size=size,
             figsize=(12, 4),
             save_name='setting.pdf')


def draw_confidence_ratio():
    pass


def random_sampler_gradients(classes=3, tau_p=1, tau_q=4):
    torch.set_printoptions(precision=4, linewidth=1000, sci_mode=False)
    logits = torch.randint(0, 10, (classes,)).float()  # * 10
    # logits.requires_grad = True

    p = (logits / tau_p).softmax(0)
    q = (logits / tau_q).softmax(0)
    loss = - (p * q.log()).sum()

    kd_grad = (p - q) / tau_q
    em_grad = (q.log() + loss) / tau_p * p

    total_grad = -(kd_grad + em_grad)

    # loss.backward()

    print(p)
    print(q)
    print(loss)
    print(kd_grad)
    print(em_grad)

    diff = ((kd_grad > 0) & (em_grad < 0)) | ((kd_grad < 0) & (em_grad > 0))
    print(diff.sum(), '/', classes)

    # print(total_grad)
    # print(logits.grad)


def draw_alpha():
    sns.set(style="whitegrid")
    domain_alphas = [
        [0.6325, 0.8239, 0.8010, 0.8050, 0.8249, 0.7451, 0.6524, 0.7557, 0.7618, 0.7866, 0.7923, 0.8607, 0.7741, 0.8994, 0.8762, 0.9005, 0.9291, 0.8769, 0.9397, 0.9530, ],
        [0.7289, 0.8553, 0.8044, 0.8788, 0.8514, 0.8629, 0.8233, 0.8695, 0.8107, 0.7654, 0.8736, 0.8477, 0.8370, 0.8282, 0.8709, 0.8733, 0.9116, 0.8890, 0.9365, 0.9563, ],
        [0.5710, 0.7753, 0.7103, 0.8501, 0.8125, 0.7815, 0.6591, 0.7740, 0.7496, 0.7965, 0.7771, 0.8636, 0.7724, 0.8752, 0.8338, 0.8597, 0.8743, 0.8856, 0.8341, 0.9496, ],
        [0.5000, 0.6505, 0.5424, 0.6620, 0.5785, 0.5666, 0.5459, 0.5544, 0.6023, 0.6177, 0.5895, 0.6976, 0.5837, 0.7647, 0.7736, 0.7898, 0.8404, 0.7189, 0.8512, 0.9276, ]
    ]
    domains = ['Art', 'Cartoon', 'Photo', 'Sketch']
    size = 20

    def plot_bar():
        bar_plot(domain_alphas,
                 bar_names=domains,
                 figsize=(6, 4),
                 offset_between_bars=0.03,
                 # ylim=[0, 5],
                 title_size=size - 3,
                 label_size=size - 1,
                 tick_size=size - 3,
                 legend_size=size - 5,
                 # colors=["#f89588"] + ["#7cd6cf"],
                 ylim=[0.5, 1],
                 y_label='Alpha',
                 x_label='BN Layers',
                 title='Values of alpha on PACS',
                 save_name='alpha_PACS.pdf')

    def plot_regression():
        colors = ['#5A9BD5', '#1B9E78', '#FF9966', '#ff585d', '#614ad3', '#feda77']

        # for i, alpha in enumerate(domain_alphas):
        #     x = np.arange(0, len(domain_alphas[0])).reshape(-1, 1)
        #     y = alpha
        #     plt.scatter(x, y, color=colors[i])
        #     reg = LinearRegression().fit(x, y)
        #     cof = reg.score(x, y)
        #     print(cof)
        #
        #     x = np.linspace(x.min(), x.max())
        #     y = reg.predict(x.reshape(-1, 1)).reshape(-1)
        #     plt.plot(x, y, '-', label='regression', color=colors[i])
        # plt.show()
        # break

    regression_plot(domain_alphas,
                    line_names=domains,
                    line_width=3,
                    figsize=(8, 4),
                    title_size=size - 3,
                    label_size=size - 1,
                    tick_size=size - 3,
                    legend_size=size - 5,
                    ylim=[0.46, 1],
                    y_label='Alpha',
                    x_label='BN Layers',
                    x_axis_names=[str(i) if i % 2 == 0 else '' for i in range(len(domain_alphas[0]))],
                    # title='Values of alpha on PACS',
                    save_name='alpha_PACS.pdf')


def run_time_comparison():
    algorithms = ['NonAdapted', 'ARM', 'AdaBN', 'SLR', 'Tent', 'LAME']
    rum_times = []


def draw_small_dataset():
    sns.set(style="whitegrid")
    accs = {
        'Tent': [80.79, 81.00, 81.24, 81.75, 82.41, 82.95, 83.52],
        'SLR': [81.33, 81.99, 82.55, 83.46, 84.26, 84.79, 85.15],
        'DomainAdaptor-T': [85.04] * 7,
    }
    x_axis = [f'{64 * 2 ** i}' for i in range(0, 7)]
    line_plot(
        list(accs.values()),
        figsize=(7, 3),
        line_names=list(accs.keys()),
        x_label='Subset Size',
        y_label='Acc (%)',
        ylim=[80, 86],
        legend_size=12,
        text_x_offset=-0.25,
        text_size=11,
        text_y_offset=0.1,
        x_axis_names=x_axis,
        save_name='small_dataset.pdf'
    )


def draw_stats_changes_after_finetune():
    PACS = [
        [
            [0.0000, 0.3897, 0.0505, 0.0874, 0.0283, 0.0551, 0.0459, 0.0199, 0.0908, 0.0227, 0.1402, 0.0330, 0.0058, 0.1470, 0.0172, 0.1461, 0.0230, 0.0049, 0.2902, 0.0009, ],
            [0.0000, 0.1629, 0.1265, 0.3677, 0.0589, 0.3431, 0.0643, 0.1169, 0.1453, 0.0523, 0.6055, 0.4180, 0.0189, 0.4744, 0.2471, 1.2687, 0.0413, 0.1058, 0.3140, 0.0955, ]
            [0.0000, 0.2492, 0.0554, 0.0516, 0.0267, 0.0496, 0.0412, 0.0117, 0.0942, 0.0215, 0.1473, 0.0322, 0.0059, 0.1479, 0.0178, 0.1534, 0.0235, 0.0049, 0.3242, 0.0010,],
            [0.0000, 1.5963, 0.1357, 0.3351, 0.0261, 0.1566, 0.0226, 0.1440, 0.0670, 0.0137, 0.1506, 0.0316, 0.0061, 0.1438, 0.0144, 0.1443, 0.0216, 0.0043, 0.3016, 0.0007, ]
        ],
        [
            [0.0000, 0.0016, 0.0004, 0.0008, 0.0002, 0.0006, 0.0002, 0.0001, 0.0003, 0.0001, 0.0010, 0.0002, 0.0000, 0.0004, 0.0001, 0.0008, 0.0001, 0.0001, 0.0018, 0.0002, ],
            [0.0000, 0.0015, 0.0006, 0.0007, 0.0002, 0.0005, 0.0002, 0.0001, 0.0005, 0.0001, 0.0016, 0.0003, 0.0001, 0.0005, 0.0001, 0.0008, 0.0002, 0.0002, 0.0009, 0.0004, ],
            [0.0000, 0.0025, 0.0005, 0.0011, 0.0003, 0.0009, 0.0002, 0.0002, 0.0005, 0.0001, 0.0019, 0.0004, 0.0001, 0.0008, 0.0002, 0.0018, 0.0003, 0.0002, 0.0029, 0.0009, ],
            [0.0000, 0.0022, 0.0010, 0.0010, 0.0006, 0.0009, 0.0004, 0.0002, 0.0005, 0.0001, 0.0016, 0.0004, 0.0001, 0.0005, 0.0001, 0.0006, 0.0002, 0.0001, 0.0007, 0.0002, ],
        ],
        [
            [0.0000, 0.0199, 0.0125, 0.0334, 0.0050, 0.0361, 0.0036, 0.0101, 0.0089, 0.0014, 0.0506, 0.0207, 0.0016, 0.0175, 0.0015, 0.0456, 0.0069, 0.0058, 0.0407, 0.0211, ],
            [0.0000, 0.0408, 0.0197, 0.0505, 0.0083, 0.0499, 0.0076, 0.0161, 0.0094, 0.0015, 0.0595, 0.0205, 0.0016, 0.0198, 0.0015, 0.0415, 0.0075, 0.0053, 0.0596, 0.0309, ],
            [0.0000, 0.0401, 0.0243, 0.0525, 0.0070, 0.0580, 0.0053, 0.0162, 0.0154, 0.0022, 0.0801, 0.0373, 0.0023, 0.0513, 0.0039, 0.1184, 0.0182, 0.0150, 0.1347, 0.0450, ],
            [0.0000, 0.0121, 0.0078, 0.0129, 0.0035, 0.0196, 0.0025, 0.0048, 0.0066, 0.0009, 0.0164, 0.0090, 0.0007, 0.0119, 0.0015, 0.0234, 0.0050, 0.0031, 0.0331, 0.0255, ]
        ],
        [
            [0.0000, 0.0022, 0.0004, 0.0009, 0.0002, 0.0006, 0.0001, 0.0001, 0.0002, 0.0001, 0.0008, 0.0002, 0.0000, 0.0003, 0.0001, 0.0006, 0.0001, 0.0001, 0.0005, 0.0002, ],
            [0.0000, 0.0008, 0.0003, 0.0005, 0.0002, 0.0004, 0.0001, 0.0001, 0.0003, 0.0001, 0.0008, 0.0002, 0.0000, 0.0003, 0.0001, 0.0005, 0.0001, 0.0001, 0.0005, 0.0003, ],
            [0.0000, 0.0035, 0.0006, 0.0014, 0.0002, 0.0011, 0.0002, 0.0002, 0.0004, 0.0001, 0.0014, 0.0003, 0.0000, 0.0006, 0.0001, 0.0011, 0.0003, 0.0001, 0.0008, 0.0008, ],
            [0.0000, 0.0018, 0.0007, 0.0008, 0.0005, 0.0008, 0.0003, 0.0002, 0.0005, 0.0001, 0.0016, 0.0004, 0.0001, 0.0005, 0.0001, 0.0008, 0.0002, 0.0001, 0.0009, 0.0002, ]
        ]
    ]
    VLCS = [
        [
            [0.0000, 0.0063, 0.0029, 0.0037, 0.0011, 0.0023, 0.0008, 0.0012, 0.0016, 0.0002, 0.0051, 0.0034, 0.0002, 0.0056, 0.0019, 0.0097, 0.0009, 0.0010, 0.0068, 0.0010, ],
            [0.0000, 0.0025, 0.0022, 0.0021, 0.0009, 0.0016, 0.0006, 0.0009, 0.0013, 0.0001, 0.0033, 0.0024, 0.0001, 0.0025, 0.0004, 0.0042, 0.0007, 0.0006, 0.0172, 0.0013, ],
            [0.0000, 0.0059, 0.0022, 0.0030, 0.0004, 0.0016, 0.0007, 0.0009, 0.0014, 0.0002, 0.0037, 0.0025, 0.0001, 0.0023, 0.0004, 0.0022, 0.0003, 0.0006, 0.0023, 0.0006, ],
            [0.0000, 0.0034, 0.0015, 0.0023, 0.0005, 0.0019, 0.0008, 0.0008, 0.0018, 0.0002, 0.0039, 0.0028, 0.0002, 0.0030, 0.0002, 0.0028, 0.0003, 0.0006, 0.0030, 0.0005, ]
        ],
        [
            [0.0000, 0.0003, 0.0001, 0.0001, 0.0000, 0.0001, 0.0000, 0.0000, 0.0000, 0.0000, 0.0001, 0.0000, 0.0000, 0.0001, 0.0000, 0.0001, 0.0000, 0.0000, 0.0001, 0.0001, ],
            [0.0000, 0.0003, 0.0001, 0.0001, 0.0000, 0.0001, 0.0000, 0.0000, 0.0000, 0.0000, 0.0001, 0.0000, 0.0000, 0.0001, 0.0000, 0.0001, 0.0000, 0.0000, 0.0001, 0.0001, ],
            [0.0000, 0.0001, 0.0000, 0.0001, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0001, 0.0000, 0.0000, 0.0000, 0.0000, 0.0001, 0.0000, 0.0000, 0.0002, 0.0001, ],
            [0.0000, 0.0005, 0.0001, 0.0002, 0.0000, 0.0001, 0.0000, 0.0000, 0.0001, 0.0000, 0.0002, 0.0000, 0.0000, 0.0001, 0.0000, 0.0001, 0.0000, 0.0000, 0.0002, 0.0001, ],
        ]

    ]

    OH = [
        [
            [0.0000, 0.0089, 0.0043, 0.0030, 0.0010, 0.0019, 0.0015, 0.0015, 0.0027, 0.0002, 0.0065, 0.0029, 0.0003, 0.0017, 0.0002, 0.0024, 0.0003, 0.0006, 0.0019, 0.0003, ],
            [0.0000, 0.0128, 0.0027, 0.0070, 0.0035, 0.0055, 0.0008, 0.0032, 0.0028, 0.0006, 0.0093, 0.0037, 0.0004, 0.0021, 0.0002, 0.0049, 0.0005, 0.0010, 0.0051, 0.0004, ],
            [0.0000, 0.0204, 0.0190, 0.0059, 0.0037, 0.0051, 0.0012, 0.0025, 0.0020, 0.0003, 0.0070, 0.0031, 0.0003, 0.0019, 0.0005, 0.0030, 0.0005, 0.0007, 0.0071, 0.0004, ],
            [0.0000, 0.0033, 0.0012, 0.0022, 0.0008, 0.0018, 0.0008, 0.0009, 0.0013, 0.0001, 0.0032, 0.0018, 0.0001, 0.0016, 0.0002, 0.0021, 0.0004, 0.0003, 0.0056, 0.0004, ]
        ],
        [
            [0.0000, 0.0001, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0001, 0.0000, 0.0000, 0.0000, 0.0000, 0.0001, 0.0000, 0.0000, 0.0001, 0.0000, ],
            [0.0000, 0.0001, 0.0001, 0.0001, 0.0000, 0.0001, 0.0000, 0.0000, 0.0000, 0.0000, 0.0001, 0.0000, 0.0000, 0.0001, 0.0000, 0.0001, 0.0000, 0.0000, 0.0000, 0.0000, ],
            [0.0000, 0.0003, 0.0002, 0.0002, 0.0001, 0.0002, 0.0000, 0.0000, 0.0001, 0.0000, 0.0003, 0.0001, 0.0000, 0.0001, 0.0000, 0.0002, 0.0000, 0.0000, 0.0001, 0.0000, ],
            [0.0000, 0.0002, 0.0001, 0.0001, 0.0000, 0.0001, 0.0000, 0.0000, 0.0001, 0.0000, 0.0002, 0.0000, 0.0000, 0.0001, 0.0000, 0.0002, 0.0000, 0.0000, 0.0001, 0.0001, ]
        ]
    ]

    MDN = [

    ]

    src2batch = [
        [1.0306, 0.2688, 1.1479, 3.2507],
        [0.0959, 0.0351, 0.0581, 0.0298],
        [0.2304, 0.0418, 0.9507, 0.0486]
    ]
    PACS = np.array(PACS)  # 4 methods x 4 domains x 20 layers
    print(PACS.mean(1).mean(1))


# performance_degradation()
# confience()
# draw_small_dataset()
# plot_class_Norm_and_number()
# batch_size()
# different_T()
# draw_losses_with_confidence()
# draw_continuous_adaptation()
# draw_alpha()
# for c in range(2, 140, 8):
#     random_sampler_gradients(classes=c)
#     print()
draw_stats_changes_after_finetune()
